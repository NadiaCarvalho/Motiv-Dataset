{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaCarvalho/Motiv-Dataset/blob/main/Dataset_Construction_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fHF6YFl05jp"
      },
      "source": [
        "### Note:\n",
        "Run Setup first, then restart and run all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnvLr9SBlfyG"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lEj9QhROOGJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive/AdapExperiments/Performance/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCHUBNe5dfVR"
      },
      "outputs": [],
      "source": [
        "import altair\n",
        "\n",
        "print(altair.__version__)\n",
        "!pip install --upgrade altair anywidget -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiZVUVeQliSj"
      },
      "source": [
        "# Functions and Loadings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IDp15lN79pl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Import Model\n",
        "import torch\n",
        "model1 = torch.jit.load(f'/content/drive/MyDrive/AdapExperiments/models/RAVE_MODELS/musicnet.ts')\n",
        "model2 = torch.jit.load(f'/content/drive/MyDrive/AdapExperiments/models/RAVE_MODELS/voice_vocalset_b2048_r48000_z16.ts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jgGyhczjK1qT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Get Latent Space\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_audio(path, model, sr=48000, samplingSize=None, normalize=False):\n",
        "  audio, sr = librosa.load(path, sr=sr)\n",
        "\n",
        "  if normalize:\n",
        "    audio = librosa.util.normalize(audio)\n",
        "\n",
        "  x = torch.from_numpy(audio)\n",
        "  x = x[None, None, :]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    z = model.encode(x)\n",
        "    reconst = model(x).squeeze(0).detach().numpy()\n",
        "\n",
        "  lat = z.squeeze(0).detach().numpy().T\n",
        "\n",
        "  if samplingSize is None or samplingSize == 1:\n",
        "    return x, z, reconst, lat\n",
        "\n",
        "  # regroup per sample samples\n",
        "  lat_df = pd.DataFrame(lat)\n",
        "  lat_ss = lat_df.groupby(np.arange(len(lat_df)) // samplingSize).mean().values\n",
        "\n",
        "  m = lat_ss.shape[0]\n",
        "  n = int(np.ceil(audio.shape[0] / lat_ss.shape[0]))\n",
        "  pads = m*n - audio.shape[0]\n",
        "\n",
        "  reconstp = reconst.squeeze(0)\n",
        "\n",
        "  if pads > 0:\n",
        "    samples = np.pad(audio.astype(float), (0, pads), mode='constant', constant_values=0)\n",
        "    reconstp = np.pad(reconstp.astype(float), (0, pads + (audio.shape[0] - reconstp.shape[0])), mode='constant', constant_values=0)\n",
        "  else:\n",
        "    samples = audio[:pads]\n",
        "    reconstp = reconstp[:pads]\n",
        "\n",
        "  samples = np.reshape(np.asarray(samples), (m,n))\n",
        "  rsamples = np.reshape(np.asarray(reconstp), (m,n))\n",
        "\n",
        "  return samples, z, rsamples, lat_ss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V8kSjG8xF0kq"
      },
      "outputs": [],
      "source": [
        "#@title Functions\n",
        "import itertools\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding\n",
        "\n",
        "import altair as alt\n",
        "from altair import datum\n",
        "print(alt.__version__) # above 5.0\n",
        "\n",
        "def get_dimensionality_reduction(algorithm, latent_spaces, n_components=2):\n",
        "    if algorithm == 'pca':\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        pca = PCA(n_components=n_components)\n",
        "        predictions = pca.fit_transform(\n",
        "            StandardScaler().fit_transform(latent_spaces))\n",
        "    elif algorithm == 'tsne':\n",
        "        tsne = TSNE(n_components=n_components, perplexity=30 if len(latent_spaces) > 30 else len(latent_spaces)-1)\n",
        "        predictions = tsne.fit_transform(np.asarray(latent_spaces))\n",
        "    elif algorithm == 'mds':\n",
        "        mds = MDS(n_components=n_components, normalized_stress=\"auto\", max_iter=100)\n",
        "        predictions = mds.fit_transform(latent_spaces)\n",
        "    elif algorithm == 'isomap':\n",
        "        iso = Isomap(n_components=n_components)\n",
        "        predictions = iso.fit_transform(latent_spaces)\n",
        "    elif algorithm == 'lle':\n",
        "        lle = LocallyLinearEmbedding(n_components=n_components)\n",
        "        predictions = lle.fit_transform(latent_spaces)\n",
        "    else:\n",
        "        import umap\n",
        "        reducer = umap.UMAP(n_components=n_components)\n",
        "        predictions = reducer.fit_transform(latent_spaces)\n",
        "    return predictions\n",
        "\n",
        "def get_vegas_data(predictions, labels, radius=.01):\n",
        "  source = {\n",
        "      'source': ['_'.join(l.split('_')[0:-1]) for l in labels],\n",
        "      'stype': [l.split('_')[0] for l in labels],\n",
        "      'id_s': [l.split('_')[-1] for l in labels],\n",
        "      'id_s_o': [int(l.split('_')[-1]) for l in labels],\n",
        "  }\n",
        "  if len(predictions.shape) == 1:\n",
        "      source['x'] = [int(l.split('_')[-1]) for l in labels]\n",
        "      source['y'] = predictions\n",
        "  else:\n",
        "      source['x'] = predictions[:, 0]\n",
        "      source['y'] = predictions[:, 1]\n",
        "\n",
        "  return pd.DataFrame(source)\n",
        "\n",
        "def get_plot2(latsS, lats0, lats, dimensions=[0, 1], algorithm='lle'):\n",
        "  if len(latsS) == 0:\n",
        "    all_elements = np.vstack(tuple([lats0] + list(lats.values())))\n",
        "  else:\n",
        "    all_elements = np.vstack(tuple([latsS, lats0] + list(lats.values())))\n",
        "  #print(all_elements.shape)\n",
        "\n",
        "  if len(dimensions) > 2:\n",
        "      points = get_dimensionality_reduction(algorithm, all_elements[:, dimensions], 2)\n",
        "  elif len(dimensions) == 2:\n",
        "      points = all_elements[:, dimensions]\n",
        "  elif len(dimensions) == 1:\n",
        "      points = all_elements[:, dimensions[0]]\n",
        "  else:\n",
        "      points = get_dimensionality_reduction(algorithm, all_elements, 2)\n",
        "\n",
        "  def flatten_comprehension(matrix):\n",
        "    return [item for row in matrix for item in row]\n",
        "\n",
        "  return get_vegas_data(points, labels=[f'Score_{i}' for i in list(range(len(latsS)))]\n",
        "                        + [f'Original_{i}' for i in list(range(len(lats0)))] +\n",
        "   flatten_comprehension([[f'{n}_{i}' for i in list(range(len(l)))] for n, l in lats.items()]))\n",
        "\n",
        "def get_plot_all_equal(lats, dimensions=[0, 1], algorithm='lle'):\n",
        "  all_elements = np.vstack(list(lats.values()))\n",
        "  print(all_elements.shape)\n",
        "\n",
        "  if len(dimensions) > 2:\n",
        "      points = get_dimensionality_reduction(algorithm, all_elements[:, dimensions], 2)\n",
        "  elif len(dimensions) == 2:\n",
        "      points = all_elements[:, dimensions]\n",
        "  elif len(dimensions) == 1:\n",
        "      points = all_elements[:, dimensions[0]]\n",
        "  else:\n",
        "      points = get_dimensionality_reduction(algorithm, all_elements, 2)\n",
        "\n",
        "  def flatten_comprehension(matrix):\n",
        "    return [item for row in matrix for item in row]\n",
        "\n",
        "  return get_vegas_data(points, labels=flatten_comprehension([[f'{n}_{i}' for i in list(range(len(l)))] for n, l in lats.items()]))\n",
        "\n",
        "\n",
        "def plot_vegas_data(source, titleX=\"x\", titleY=\"y\"):\n",
        "\n",
        "  search_input = alt.selection_point(\n",
        "      fields=['id_s'],\n",
        "      empty=False,  # Start with no points selected\n",
        "      bind=alt.binding(\n",
        "          input='search',\n",
        "          placeholder='Grain',\n",
        "          name='Search ',\n",
        "      )\n",
        "  )\n",
        "  selection = alt.selection_interval(bind='scales')\n",
        "  highlight = alt.selection_point(\n",
        "    on=\"pointerover\", fields=[\"source\"], nearest=True\n",
        "  )\n",
        "\n",
        "  param_checkbox = alt.param(\n",
        "      bind=alt.binding_checkbox(name='View Trajectories:'),\n",
        "      name='Trajectories')\n",
        "  score_grain_checkbox = alt.param(\n",
        "      bind=alt.binding_checkbox(name='View Score Grains:'),\n",
        "      value=True,\n",
        "      name='ScoreGrain')\n",
        "\n",
        "  chart = alt.Chart(source[source['source'] != 'Score']).encode(\n",
        "    x=alt.X('x:Q', title=titleX),\n",
        "    y=alt.Y('y:Q', title=titleY),\n",
        "    color='stype:N',\n",
        "    tooltip=['source:N', 'id_s:N']\n",
        "  )\n",
        "  points = chart.mark_circle().encode(\n",
        "    opacity=alt.condition(\n",
        "        search_input,\n",
        "        alt.value(1),\n",
        "        alt.value(0.3)\n",
        "    )\n",
        "  ).add_params(\n",
        "      selection,\n",
        "      highlight,\n",
        "      search_input\n",
        "  ).properties(\n",
        "      name='Grains',width=800,height=500\n",
        "  )\n",
        "\n",
        "  # create marks for first and last points\n",
        "  lines = chart.mark_line().encode(\n",
        "      size=alt.condition(~highlight, alt.value(1), alt.value(2)),\n",
        "      opacity=alt.condition(\n",
        "        param_checkbox,\n",
        "        alt.value(1),\n",
        "        alt.value(0)\n",
        "    ),\n",
        "    strokeDash='source',\n",
        "    order=\"id_s_o:Q\",\n",
        "  ).add_params(\n",
        "      param_checkbox\n",
        "  ).properties(\n",
        "      name='Trajectories',width=800,height=500\n",
        "  )\n",
        "  circles = chart.mark_circle(size=60).encode(\n",
        "      opacity=alt.condition(\n",
        "        param_checkbox,\n",
        "        alt.value(1),\n",
        "        alt.value(0)\n",
        "      )\n",
        "  )\n",
        "  arrows = chart.mark_point(shape='wedge', size=60).encode(\n",
        "      opacity=alt.condition(\n",
        "        param_checkbox,\n",
        "        alt.value(1),\n",
        "        alt.value(0)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  chartS = alt.Chart(source[source['source'] == 'Score']).encode(\n",
        "    x=alt.X('x:Q', title=titleX),\n",
        "    y=alt.Y('y:Q', title=titleY),\n",
        "    color='source:N',\n",
        "    tooltip=['source:N', 'id_s:N']\n",
        "  )\n",
        "  pointsS = chartS.mark_circle().encode(\n",
        "    opacity=alt.condition(\n",
        "        score_grain_checkbox,\n",
        "        alt.value(.2),\n",
        "        alt.value(0)\n",
        "    )\n",
        "  ).transform_filter(\n",
        "      datum.source == 'Score'\n",
        "  ).add_params(\n",
        "      selection,\n",
        "      score_grain_checkbox\n",
        "  ).properties(\n",
        "      name='Score',width=800,height=500\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  return pointsS, points, lines, circles, arrows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL6NocyZBW4A"
      },
      "source": [
        "# Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "jZSHIghnIWkY"
      },
      "outputs": [],
      "source": [
        "#@title Show Graphs\n",
        "\n",
        "import glob\n",
        "import time\n",
        "import tqdm.notebook as tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "score_audio, original_audio, selected_audios, model = None, None, None, model2\n",
        "\n",
        "model_selector = widgets.Select(options=['MusicNet', 'VocalSet'], description='Model', index=1)\n",
        "experiment_selector = widgets.IntSlider(1, 1, 3, 1, description='Experiment')\n",
        "phrase_selector = widgets.IntSlider(1, 1, 3, 1, description='Phrase')\n",
        "\n",
        "opts = sorted([x for x in glob.glob(f'PhraseRecordings/Experiment1/phrase_1/*.wav') if 'original' not in x])\n",
        "selector = widgets.SelectMultiple(options=opts,\n",
        "                                  value=opts[0:1], description='Phrase Type',\n",
        "                                  layout=widgets.Layout(width='75%', height='100px'),\n",
        "                                  style={'button_color': 'red'},)\n",
        "sample_size = widgets.IntSlider(4, 1, 12, 1.0, description='Spl Group')\n",
        "dimension_selector = widgets.SelectMultiple(options=list(range(16)), description='Dimensions', value=list(range(16)))\n",
        "\n",
        "startA = widgets.Button(description='Get Audios')\n",
        "startb = widgets.Button(description='Start')\n",
        "\n",
        "def on_change_exp(change):\n",
        "    global selector\n",
        "    phrase_selector.max = 4 if experiment_selector.value == 2 else 3\n",
        "    exp_name = 'Experiment1' if experiment_selector.value == 1 else 'Experiment2' if experiment_selector.value == 2 else 'Experiment1-Multiple'\n",
        "    opts = sorted([x for x in glob.glob(f'PhraseRecordings/{exp_name}/phrase_{phrase_selector.value}/*.wav') if 'original' not in x and 'electronics' not in x])\n",
        "    selector.options = opts\n",
        "\n",
        "def on_change(change):\n",
        "    global selector\n",
        "    exp_name = 'Experiment1' if experiment_selector.value == 1 else 'Experiment2' if experiment_selector.value == 2 else 'Experiment1-Multiple'\n",
        "    opts = sorted([x for x in glob.glob(f'PhraseRecordings/{exp_name}/phrase_{phrase_selector.value}/*.wav') if 'original' not in x and 'electronics' not in x])\n",
        "    selector.options = opts\n",
        "    selector.value = opts[0:1]\n",
        "\n",
        "def on_change_m(change):\n",
        "    global model\n",
        "    if model_selector.value == 'MusicNet':\n",
        "        model = model1\n",
        "    else:\n",
        "        model = model2\n",
        "\n",
        "    dimension_selector.options = list(range(16))\n",
        "\n",
        "experiment_selector.observe(on_change_exp)\n",
        "phrase_selector.observe(on_change)\n",
        "model_selector.observe(on_change_m)\n",
        "\n",
        "display(widgets.HBox([model_selector, dimension_selector]))\n",
        "display(widgets.HBox([experiment_selector, phrase_selector]))\n",
        "display(selector)\n",
        "display(sample_size)\n",
        "display(widgets.HBox([startA, startb]))\n",
        "\n",
        "out1 = widgets.Output(layout={'border': '0px solid black', 'padding': '.5em', 'width': '90%'})\n",
        "\n",
        "def on_startA(change):\n",
        "    global score_audio, original_audio, selected_audios\n",
        "\n",
        "    with out1:\n",
        "      clear_output(wait=False)\n",
        "      time.sleep(1)\n",
        "\n",
        "      print(f'\\nStarting latent space generation for Phrase {phrase_selector.value} with samplingSize {sample_size.value}')\n",
        "\n",
        "      print('Getting original audios')\n",
        "      if experiment_selector.value == 1:\n",
        "          original_audio = get_audio(f'PhraseRecordings/Experiment{experiment_selector.value}/phrase_{phrase_selector.value}/phrase_{phrase_selector.value}_original.wav', model, samplingSize=sample_size.value)\n",
        "          print('Original Phrase Done')\n",
        "          score_audio = get_audio('Lamento-Sax-Complete-Audio/Lamento-Take1.wav', model, samplingSize=sample_size)\n",
        "          print('Original Score Done')\n",
        "      elif experiment_selector.value == 3:\n",
        "          original_audio = get_audio(f'PhraseRecordings/Experiment1-Multiple/phrase_{phrase_selector.value}/OR_T02.wav', model, samplingSize=sample_size.value)\n",
        "          print('Original Phrase Done')\n",
        "          score_audio = [[],[],[],[]] #get_audio('Lamento-Sax-Complete-Audio/Lamento-Take3.wav', model, samplingSize=sample_size)\n",
        "          print('Original Score Done')\n",
        "      else:\n",
        "          original_audio = get_audio(f'PhraseRecordings/Experiment{experiment_selector.value}/phrase_{phrase_selector.value}/phrase_{phrase_selector.value}_electronics.wav', model, samplingSize=sample_size.value)\n",
        "          print('Original Phrase Done')\n",
        "          score_audio = get_audio('Audios/Lamento_Villa_Rojo.mpeg', model, samplingSize=sample_size)\n",
        "          print('Original Score Done')\n",
        "\n",
        "      print(f'Getting selected audios')\n",
        "      if experiment_selector.value == 3:\n",
        "        selected_audios = {v.split('/')[-1][:-4]:get_audio(v, model, samplingSize=sample_size.value) for v in tqdm(selector.value)}\n",
        "      else:\n",
        "        selected_audios = {'_'.join(v.split('/')[-1].split('_')[2:])[:-4]:get_audio(v, model, samplingSize=sample_size.value) for v in tqdm(selector.value)}\n",
        "\n",
        "def on_change2(change):\n",
        "    global model, original_audio, selected_audios\n",
        "\n",
        "    with out1:\n",
        "        clear_output(wait=False)\n",
        "        time.sleep(1)\n",
        "\n",
        "        print(f'Generating latent space visualization for dimensions {dimension_selector.value}')\n",
        "\n",
        "        source = get_plot2(score_audio[3], original_audio[3], {v:sa[3] for v, sa in selected_audios.items()}, dimension_selector.value, algorithm='tsne')\n",
        "        print('Extracted Source')\n",
        "\n",
        "        titleY = f\"Latent Dimension {dimension_selector.value[0]}\" if len(dimension_selector.value) == 1 else f\"Latent Dimension {dimension_selector.value[1]}\" if len(dimension_selector.value) == 2 else \"t-SNE 2\"\n",
        "        titleX = f\"Unit\" if len(dimension_selector.value) == 1 else f\"Latent Dimension {dimension_selector.value[0]}\" if len(dimension_selector.value) == 2 else \"t-SNE 1\"\n",
        "        scorep, points, lines, circleSt, arrowEnd = plot_vegas_data(source, titleX, titleY)\n",
        "        print('Extracted Plot')\n",
        "\n",
        "        max_R = max(source[source['source'] != 'Score']['id_s'].apply(func=lambda x: int(x)))\n",
        "        jchart = alt.JupyterChart(alt.layer(points + lines + circleSt.transform_filter((datum.id_s_o == 0)) + arrowEnd.transform_filter((datum.id_s_o == max_R))))\n",
        "        jchart.chart = jchart.chart.properties(width=800,height=500).interactive()\n",
        "\n",
        "        rangeR = widgets.IntRangeSlider(value=[0,max_R], min=0, max=max_R)\n",
        "        def on_change_range_R(change):\n",
        "          m = rangeR.value[0]\n",
        "          n = rangeR.value[1]\n",
        "\n",
        "          if len(dimension_selector.value) == 1:\n",
        "            jchart.chart =   (\n",
        "                points.encode(\n",
        "                x=alt.X('x:Q', title=titleX, scale=alt.Scale(domain=[m-1,n+1], nice=False))).transform_filter((datum.id_s_o >= m) & (datum.id_s_o <= n))\n",
        "                + lines.transform_filter((datum.id_s_o >= m) & (datum.id_s_o <= n))\n",
        "                ) + circleSt.transform_filter((datum.id_s_o == m)) + arrowEnd.transform_filter((datum.id_s_o == n))\n",
        "          else:\n",
        "            jchart.chart =   (\n",
        "                points.transform_filter((datum.id_s_o >= m) & (datum.id_s_o <= n))\n",
        "                + lines.transform_filter((datum.id_s_o >= m) & (datum.id_s_o <= n))\n",
        "                ) + circleSt.transform_filter((datum.id_s_o == m)) + arrowEnd.transform_filter((datum.id_s_o == n))\n",
        "\n",
        "          if len(dimension_selector.value) == 1:\n",
        "            jchart.chart\n",
        "\n",
        "          jchart.chart = jchart.chart.properties(width=800,height=500).interactive()\n",
        "\n",
        "        rangeR.observe(on_change_range_R)\n",
        "        display(rangeR)\n",
        "        time.sleep(1)\n",
        "        display(jchart)\n",
        "\n",
        "startA.on_click(on_startA)\n",
        "startb.on_click(on_change2)\n",
        "display(out1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL6ZAD6uBkFm"
      },
      "source": [
        "# Dataset Construction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Annotations\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "annotations = pd.read_csv('../AudioMostly/annotations.csv', header=0, index_col=[0,1])"
      ],
      "metadata": {
        "id": "Y9HsDtHTFYre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import h5py\n",
        "import librosa\n",
        "import tqdm.notebook as tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_audios = glob.glob(f'PhraseRecordings/Experiment1-Multiple/*/*.wav')\n",
        "model = model2\n",
        "\n",
        "with h5py.File('Motiv.hdf5', 'w') as f:\n",
        "\n",
        "  for audio_path in tqdm.tqdm(all_audios):\n",
        "    sample_name = audio_path.split('/')[-1][:-4]\n",
        "\n",
        "    sax = sample_name.split('_')[1]\n",
        "    phrase = int(sample_name.split('_')[0][2:])\n",
        "    motion = sample_name.split('_')[2]\n",
        "\n",
        "    # Load audio file (replace with real path)\n",
        "    waveform, sr = librosa.load(audio_path, sr=48000)\n",
        "\n",
        "    # Latent Vector\n",
        "    x = torch.from_numpy(waveform)[None, None, :]\n",
        "    with torch.no_grad():\n",
        "      z = model.encode(x)\n",
        "    lat = z.squeeze(0).detach().numpy().T\n",
        "\n",
        "    # Example annotation\n",
        "\n",
        "    try:\n",
        "      annotation = annotations.loc[(sax, motion)].iloc[phrase-1]\n",
        "    except:\n",
        "      annotation = 'None'\n",
        "\n",
        "    # Store audio\n",
        "    f.create_dataset(f\"audio_samples/{sample_name}\", data=waveform)\n",
        "\n",
        "    # Store latent space\n",
        "    f.create_dataset(f\"latent_vectors/{sample_name}\", data=lat)\n",
        "\n",
        "    # Store annotation\n",
        "    f.attrs[f\"annotations/{sample_name}\"] = annotation\n",
        "\n",
        "    # Store reference to musical score image (file saved separately)\n",
        "    f.attrs[f\"musical_score/{sample_name}\"] = f\"{sample_name}.png\""
      ],
      "metadata": {
        "id": "mlnjnrEaCpfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VnvLr9SBlfyG",
        "mL6NocyZBW4A"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}